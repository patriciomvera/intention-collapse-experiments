{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intention Collapse: Experiment 4.1 (v2 - Corrected)\n",
    "## Correlating Intention Metrics with Reasoning Accuracy\n",
    "\n",
    "**Version 2.0** - Includes fixes for:\n",
    "- ✅ Activation extraction bug (size mismatch)\n",
    "- ✅ Length bias control (babble condition)\n",
    "- ✅ Layer-wise dim_eff analysis\n",
    "- ✅ Sanity checks for answer extraction\n",
    "- ✅ Robust error handling throughout\n",
    "\n",
    "### Metrics Implemented\n",
    "1. **Intention Entropy** $H_{int}(I)$: Shannon entropy of next-token distribution\n",
    "2. **Effective Dimensionality** $dim_{eff}(I)$: PCA-based dimensionality of hidden activations\n",
    "3. **Latent Recoverability** $Recov(I; Z)$: Linear probe accuracy for predicting correctness\n",
    "\n",
    "### Experimental Conditions\n",
    "- **Baseline**: Zero-shot (direct answer)\n",
    "- **Enhanced**: Chain-of-thought reasoning\n",
    "- **Babble**: Length-matched negative control (to rule out length bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate bitsandbytes\n",
    "!pip install -q datasets scikit-learn scipy\n",
    "!pip install -q matplotlib seaborn tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected. Please enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hugging Face token\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"✓ Loaded HF_TOKEN from Colab Secrets\")\n",
    "except:\n",
    "    import getpass\n",
    "    HF_TOKEN = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "    print(\"✓ Token entered manually\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model_name': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    'quantization': '4bit',\n",
    "    'extraction_layers': [27, 28, 29, 30, 31],  # Last 5 layers\n",
    "    \n",
    "    # Dataset settings\n",
    "    'dataset': 'gsm8k',\n",
    "    'subset_size': 200,  # Adjust based on time/resources\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Generation settings\n",
    "    'max_new_tokens_baseline': 50,\n",
    "    'max_new_tokens_cot': 512,\n",
    "    'max_new_tokens_babble': 512,  # Match CoT length for fair comparison\n",
    "    'temperature': 0.0,  # Greedy decoding\n",
    "    \n",
    "    # Metric settings\n",
    "    'variance_threshold': 0.90,\n",
    "    'entropy_top_k': 100,\n",
    "    'probe_regularization': 1.0,\n",
    "    \n",
    "    # Output settings\n",
    "    'output_dir': 'results',\n",
    "    'save_figures': True,\n",
    "}\n",
    "\n",
    "# Prompt templates for each condition\n",
    "PROMPTS = {\n",
    "    'baseline': \"\"\"Solve this math problem. Give only the final numerical answer.\n",
    "\n",
    "Problem: {question}\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    'enhanced': \"\"\"Solve this math problem step by step. Show your reasoning, then give the final answer after ####.\n",
    "\n",
    "Problem: {question}\n",
    "Solution:\"\"\",\n",
    "    \n",
    "    # Negative control: generates long text without reasoning\n",
    "    # This controls for the \"length bias\" critique\n",
    "    'babble': \"\"\"Given this math problem, write a long stream of consciousness about numbers, \n",
    "calculations, and mathematical concepts. Do NOT solve the problem - just write \n",
    "loosely related mathematical musings for about 100 words.\n",
    "\n",
    "Problem: {question}\n",
    "Stream of consciousness:\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Dataset: {CONFIG['dataset']} ({CONFIG['subset_size']} problems)\")\n",
    "print(f\"Conditions: baseline, enhanced (CoT), babble (negative control)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Core Implementation (with fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MathProblem:\n",
    "    \"\"\"Container for a math problem from GSM8K.\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    final_answer: str\n",
    "    idx: int\n",
    "\n",
    "@dataclass\n",
    "class IntentionMetrics:\n",
    "    \"\"\"Container for intention metrics for a single example.\"\"\"\n",
    "    entropy: float\n",
    "    dim_eff: int = 0\n",
    "    recoverability: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Results from a single problem evaluation.\"\"\"\n",
    "    problem_idx: int\n",
    "    condition: str\n",
    "    question: str\n",
    "    ground_truth: str\n",
    "    model_output: str\n",
    "    extracted_answer: str\n",
    "    is_correct: bool\n",
    "    metrics: IntentionMetrics\n",
    "    activations: Optional[np.ndarray] = None\n",
    "    output_length: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ACTIVATION EXTRACTION (FIXED VERSION)\n",
    "# =============================================================================\n",
    "# Key fix: Capture only the last token at each generation step\n",
    "# This prevents the size mismatch error when concatenating\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"\n",
    "    Extract hidden state activations from specified transformer layers.\n",
    "    \n",
    "    FIXED: Collapses to last token immediately to avoid size mismatch\n",
    "    when concatenating tensors from different generation steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layers: List[int]):\n",
    "        self.model = model\n",
    "        self.layers = sorted(layers)\n",
    "        self._hooks = []\n",
    "        self._activations = {l: [] for l in layers}\n",
    "        self._is_capturing = False\n",
    "    \n",
    "    def _get_layer_module(self, layer_idx: int):\n",
    "        \"\"\"Get the module for a specific layer.\"\"\"\n",
    "        return self.model.model.layers[layer_idx]\n",
    "    \n",
    "    def _create_hook(self, layer_idx: int):\n",
    "        \"\"\"Create a hook function - captures only last token to avoid size mismatch.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if not self._is_capturing:\n",
    "                return\n",
    "            hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "            # FIX: Capture only the last token position\n",
    "            # This ensures all captured tensors have shape [batch, hidden_dim]\n",
    "            last_hidden = hidden_states[:, -1, :].detach().cpu()\n",
    "            self._activations[layer_idx].append(last_hidden)\n",
    "        return hook\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks on specified layers.\"\"\"\n",
    "        for layer_idx in self.layers:\n",
    "            layer_module = self._get_layer_module(layer_idx)\n",
    "            hook = layer_module.register_forward_hook(self._create_hook(layer_idx))\n",
    "            self._hooks.append(hook)\n",
    "    \n",
    "    def _remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self._hooks:\n",
    "            hook.remove()\n",
    "        self._hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self._activations = {l: [] for l in self.layers}\n",
    "    \n",
    "    @contextmanager\n",
    "    def capture(self):\n",
    "        \"\"\"Context manager for capturing activations.\"\"\"\n",
    "        self.clear()\n",
    "        self._register_hooks()\n",
    "        self._is_capturing = True\n",
    "        try:\n",
    "            yield self\n",
    "        finally:\n",
    "            self._is_capturing = False\n",
    "            self._remove_hooks()\n",
    "    \n",
    "    def get_activations(self, aggregate: str = \"last\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get extracted activations.\n",
    "        \n",
    "        Args:\n",
    "            aggregate: \"last\" for last generation step, \"mean\" for mean across steps\n",
    "            \n",
    "        Returns:\n",
    "            Array of shape (n_layers, hidden_dim)\n",
    "        \"\"\"\n",
    "        all_activations = []\n",
    "        for l in self.layers:\n",
    "            if not self._activations[l]:\n",
    "                raise ValueError(f\"No activations captured for layer {l}\")\n",
    "            \n",
    "            # Now safe to concatenate: all tensors are [batch, hidden_dim]\n",
    "            layer_acts = torch.cat(self._activations[l], dim=0)  # [n_steps, hidden_dim]\n",
    "            \n",
    "            if aggregate == \"last\":\n",
    "                # Take the last generation step (final intention state)\n",
    "                layer_acts = layer_acts[-1, :]  # [hidden_dim]\n",
    "            elif aggregate == \"mean\":\n",
    "                # Mean across all generation steps\n",
    "                layer_acts = layer_acts.mean(dim=0)  # [hidden_dim]\n",
    "            elif aggregate == \"all\":\n",
    "                # Return all steps (for trajectory analysis)\n",
    "                pass\n",
    "            \n",
    "            all_activations.append(layer_acts.numpy())\n",
    "        \n",
    "        # Stack layers: (n_layers, hidden_dim)\n",
    "        return np.stack(all_activations, axis=0)\n",
    "    \n",
    "    def get_num_steps(self) -> int:\n",
    "        \"\"\"Get number of generation steps captured.\"\"\"\n",
    "        if not self._activations[self.layers[0]]:\n",
    "            return 0\n",
    "        return len(self._activations[self.layers[0]])\n",
    "\n",
    "print(\"✓ ActivationExtractor defined (with fix for size mismatch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTENTION METRICS IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_intention_entropy(logits: torch.Tensor, top_k: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Compute intention entropy H_int(I) from logits.\n",
    "    \n",
    "    Lower entropy indicates a more \"decided\" intention state.\n",
    "    We measure this on the FIRST generated token to capture\n",
    "    the model's initial decisiveness.\n",
    "    \"\"\"\n",
    "    if logits.dim() == 2:\n",
    "        logits = logits[-1]\n",
    "    \n",
    "    if top_k > 0 and top_k < logits.size(-1):\n",
    "        top_logits, _ = torch.topk(logits, top_k)\n",
    "        probs = F.softmax(top_logits, dim=-1)\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    eps = 1e-10\n",
    "    log_probs = torch.log2(probs + eps)\n",
    "    entropy = -torch.sum(probs * log_probs).item()\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def compute_effective_dimensionality(\n",
    "    activations: np.ndarray,\n",
    "    variance_threshold: float = 0.90\n",
    ") -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute effective intention dimensionality dim_eff(I) using PCA.\n",
    "    \n",
    "    Higher dim_eff suggests a \"richer\" intention state with more\n",
    "    information being processed.\n",
    "    \"\"\"\n",
    "    # Handle different input shapes\n",
    "    if activations.ndim == 1:\n",
    "        # Single vector - can't do PCA meaningfully\n",
    "        return 1, np.array([1.0])\n",
    "    \n",
    "    if activations.ndim == 3:\n",
    "        # (n_layers, n_samples, hidden_dim) -> flatten\n",
    "        n_layers, n_samples, hidden_dim = activations.shape\n",
    "        activations = activations.reshape(n_layers * n_samples, hidden_dim)\n",
    "    \n",
    "    n_samples, n_features = activations.shape\n",
    "    n_components = min(n_samples, n_features)\n",
    "    \n",
    "    if n_components < 2:\n",
    "        return 1, np.array([1.0])\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(activations)\n",
    "    \n",
    "    # Find smallest k such that cumulative variance >= threshold\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    dim_eff = np.searchsorted(cumulative_variance, variance_threshold) + 1\n",
    "    dim_eff = min(dim_eff, n_components)\n",
    "    \n",
    "    return int(dim_eff), pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "def train_recoverability_probe(\n",
    "    activations: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    cv_folds: int = 5\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train a linear probe to measure latent recoverability Recov(I; Z).\n",
    "    \n",
    "    Higher probe accuracy than verbalized accuracy suggests the model\n",
    "    \"knows more than it says\" - information is lost during collapse.\n",
    "    \"\"\"\n",
    "    if activations.ndim == 3:\n",
    "        n_layers, n_samples, hidden_dim = activations.shape\n",
    "        activations = activations.transpose(1, 0, 2).reshape(n_samples, -1)\n",
    "    elif activations.ndim == 2 and activations.shape[0] != len(labels):\n",
    "        # Reshape if needed\n",
    "        activations = activations.reshape(len(labels), -1)\n",
    "    \n",
    "    labels = np.asarray(labels).astype(int)\n",
    "    \n",
    "    # Check if we have enough samples and class balance\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return 0.5, 0.0\n",
    "    \n",
    "    if len(labels) < cv_folds:\n",
    "        cv_folds = max(2, len(labels))\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(\n",
    "            C=1.0/CONFIG['probe_regularization'],\n",
    "            max_iter=1000,\n",
    "            random_state=CONFIG['seed'],\n",
    "            solver='lbfgs'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        cv_scores = cross_val_score(pipeline, activations, labels, cv=cv_folds)\n",
    "        return cv_scores.mean(), cv_scores.std()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Probe training failed: {e}\")\n",
    "        return 0.5, 0.0\n",
    "\n",
    "print(\"✓ Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA UTILITIES (IMPROVED)\n",
    "# =============================================================================\n",
    "\n",
    "def load_gsm8k_problems(subset_size: int, seed: int) -> List[MathProblem]:\n",
    "    \"\"\"Load GSM8K dataset.\"\"\"\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "    \n",
    "    if subset_size < len(dataset):\n",
    "        random.seed(seed)\n",
    "        indices = random.sample(range(len(dataset)), subset_size)\n",
    "        dataset = dataset.select(indices)\n",
    "    \n",
    "    problems = []\n",
    "    for idx, item in enumerate(dataset):\n",
    "        solution = item['answer']\n",
    "        match = re.search(r'####\\s*(.+?)$', solution, re.MULTILINE)\n",
    "        final_answer = match.group(1).strip().replace(',', '') if match else \"\"\n",
    "        \n",
    "        problems.append(MathProblem(\n",
    "            question=item['question'],\n",
    "            answer=solution,\n",
    "            final_answer=final_answer,\n",
    "            idx=idx\n",
    "        ))\n",
    "    \n",
    "    return problems\n",
    "\n",
    "\n",
    "def extract_answer(model_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract numerical answer from model output.\n",
    "    Improved to handle more formats.\n",
    "    \"\"\"\n",
    "    output = model_output.strip()\n",
    "    \n",
    "    # Priority patterns (most specific first)\n",
    "    patterns = [\n",
    "        r'####\\s*(-?[\\d,]+\\.?\\d*)',           # GSM8K format\n",
    "        r'[Ff]inal [Aa]nswer[:\\s]+(-?[\\d,]+\\.?\\d*)',\n",
    "        r'[Aa]nswer[:\\s]+\\$?(-?[\\d,]+\\.?\\d*)',\n",
    "        r'[Tt]he answer is[:\\s]+\\$?(-?[\\d,]+\\.?\\d*)',\n",
    "        r'[Tt]otal[:\\s]+\\$?(-?[\\d,]+\\.?\\d*)',\n",
    "        r'=\\s*\\$?(-?[\\d,]+\\.?\\d*)\\s*$',\n",
    "        r'\\$(-?[\\d,]+\\.?\\d*)\\s*$',            # Dollar amount at end\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, output)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Fallback: last number in output\n",
    "    numbers = re.findall(r'-?[\\d,]+\\.?\\d*', output)\n",
    "    numbers = [n.replace(',', '') for n in numbers if n.replace(',', '').replace('.', '').replace('-', '').isdigit()]\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_answer(predicted: str, ground_truth: str, tolerance: float = 0.01) -> bool:\n",
    "    \"\"\"\n",
    "    Check if predicted answer matches ground truth.\n",
    "    Handles both integer and float comparisons.\n",
    "    \"\"\"\n",
    "    pred_clean = predicted.strip().replace(',', '').replace('$', '').replace('%', '')\n",
    "    truth_clean = ground_truth.strip().replace(',', '').replace('$', '').replace('%', '')\n",
    "    \n",
    "    # Exact string match\n",
    "    if pred_clean == truth_clean:\n",
    "        return True\n",
    "    \n",
    "    # Numerical comparison\n",
    "    try:\n",
    "        pred_num = float(pred_clean)\n",
    "        truth_num = float(truth_clean)\n",
    "        \n",
    "        # Exact match\n",
    "        if pred_num == truth_num:\n",
    "            return True\n",
    "        \n",
    "        # Integer comparison (ignore decimals)\n",
    "        if int(pred_num) == int(truth_num):\n",
    "            return True\n",
    "        \n",
    "        # Tolerance-based match for floats\n",
    "        if truth_num != 0 and abs((pred_num - truth_num) / truth_num) < tolerance:\n",
    "            return True\n",
    "            \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(\"✓ Data utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Configure quantization\n",
    "if CONFIG['quantization'] == '4bit':\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "elif CONFIG['quantization'] == '8bit':\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "else:\n",
    "    quantization_config = None\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'], token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Loading GSM8K dataset ({CONFIG['subset_size']} problems)...\")\n",
    "\n",
    "problems = load_gsm8k_problems(\n",
    "    subset_size=CONFIG['subset_size'],\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(problems)} problems\")\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"  Question: {problems[0].question[:200]}...\")\n",
    "print(f\"  Answer: {problems[0].final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT RUNNER\n",
    "# =============================================================================\n",
    "\n",
    "def run_single_problem(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    extractor: ActivationExtractor,\n",
    "    problem: MathProblem,\n",
    "    condition: str,\n",
    "    prompt_template: str,\n",
    "    max_new_tokens: int\n",
    ") -> ExperimentResult:\n",
    "    \"\"\"\n",
    "    Run model on a single problem and extract metrics.\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = prompt_template.format(question=problem.question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with activation capture\n",
    "    activations = None\n",
    "    with torch.no_grad():\n",
    "        with extractor.capture():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=CONFIG['temperature'] if CONFIG['temperature'] > 0 else None,\n",
    "                do_sample=CONFIG['temperature'] > 0,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Get activations (now with fix)\n",
    "        try:\n",
    "            activations = extractor.get_activations(aggregate=\"last\")\n",
    "        except Exception as e:\n",
    "            # This should rarely happen now with the fix\n",
    "            print(f\"Warning: Failed to get activations: {e}\")\n",
    "            activations = None\n",
    "    \n",
    "    # Decode output\n",
    "    generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
    "    model_output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute entropy from first token\n",
    "    entropy = 0.0\n",
    "    if outputs.scores:\n",
    "        first_logits = outputs.scores[0][0]\n",
    "        entropy = compute_intention_entropy(first_logits, top_k=CONFIG['entropy_top_k'])\n",
    "    \n",
    "    # Evaluate answer (only for baseline and enhanced, not babble)\n",
    "    if condition == 'babble':\n",
    "        extracted_answer = \"\"\n",
    "        is_correct = False  # Not applicable for babble\n",
    "    else:\n",
    "        extracted_answer = extract_answer(model_output)\n",
    "        is_correct = evaluate_answer(extracted_answer, problem.final_answer)\n",
    "    \n",
    "    output_length = len(model_output.split())\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_idx=problem.idx,\n",
    "        condition=condition,\n",
    "        question=problem.question,\n",
    "        ground_truth=problem.final_answer,\n",
    "        model_output=model_output,\n",
    "        extracted_answer=extracted_answer,\n",
    "        is_correct=is_correct,\n",
    "        metrics=IntentionMetrics(entropy=entropy, dim_eff=0),\n",
    "        activations=activations,\n",
    "        output_length=output_length\n",
    "    )\n",
    "\n",
    "print(\"✓ Experiment runner defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN MAIN EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = ActivationExtractor(model, CONFIG['extraction_layers'])\n",
    "\n",
    "# Storage for results - now includes 'babble' condition\n",
    "conditions = ['baseline', 'enhanced', 'babble']\n",
    "results = {c: [] for c in conditions}\n",
    "all_activations = {c: [] for c in conditions}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING EXPERIMENT 4.1: Intention Metrics vs. Accuracy\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConditions:\")\n",
    "print(\"  - baseline: Direct answer (zero-shot)\")\n",
    "print(\"  - enhanced: Chain-of-thought reasoning\")\n",
    "print(\"  - babble: Length-matched negative control\")\n",
    "\n",
    "for condition in conditions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONDITION: {condition.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt_template = PROMPTS[condition]\n",
    "    \n",
    "    # Set max tokens based on condition\n",
    "    if condition == 'baseline':\n",
    "        max_tokens = CONFIG['max_new_tokens_baseline']\n",
    "    elif condition == 'enhanced':\n",
    "        max_tokens = CONFIG['max_new_tokens_cot']\n",
    "    else:  # babble\n",
    "        max_tokens = CONFIG['max_new_tokens_babble']\n",
    "    \n",
    "    activation_success = 0\n",
    "    \n",
    "    for problem in tqdm(problems, desc=f\"Running {condition}\"):\n",
    "        result = run_single_problem(\n",
    "            model, tokenizer, extractor,\n",
    "            problem, condition,\n",
    "            prompt_template, max_tokens\n",
    "        )\n",
    "        results[condition].append(result)\n",
    "        \n",
    "        if result.activations is not None:\n",
    "            all_activations[condition].append(result.activations)\n",
    "            activation_success += 1\n",
    "        \n",
    "        # Clear CUDA cache periodically\n",
    "        if problem.idx % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print condition summary\n",
    "    if condition != 'babble':\n",
    "        accuracy = sum(r.is_correct for r in results[condition]) / len(results[condition])\n",
    "        print(f\"\\n  Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    avg_length = np.mean([r.output_length for r in results[condition]])\n",
    "    print(f\"  Avg output length: {avg_length:.1f} words\")\n",
    "    print(f\"  Activations captured: {activation_success}/{len(problems)} ({activation_success/len(problems)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE DIM_EFF FOR ALL CONDITIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nComputing effective dimensionality...\")\n",
    "\n",
    "dim_eff_results = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    if all_activations[condition]:\n",
    "        # Stack all activations: (n_examples, n_layers, hidden_dim)\n",
    "        stacked = np.stack(all_activations[condition], axis=0)\n",
    "        n_examples, n_layers, hidden_dim = stacked.shape\n",
    "        \n",
    "        # Global dim_eff across all examples\n",
    "        flattened = stacked.reshape(n_examples * n_layers, hidden_dim)\n",
    "        global_dim_eff, explained_var = compute_effective_dimensionality(\n",
    "            flattened,\n",
    "            variance_threshold=CONFIG['variance_threshold']\n",
    "        )\n",
    "        \n",
    "        # Per-example dim_eff\n",
    "        per_example_dims = []\n",
    "        for i, result in enumerate(results[condition]):\n",
    "            if i < len(all_activations[condition]):\n",
    "                acts = all_activations[condition][i]\n",
    "                dim_eff, _ = compute_effective_dimensionality(\n",
    "                    acts,\n",
    "                    variance_threshold=CONFIG['variance_threshold']\n",
    "                )\n",
    "                result.metrics.dim_eff = dim_eff\n",
    "                per_example_dims.append(dim_eff)\n",
    "        \n",
    "        dim_eff_results[condition] = {\n",
    "            'global': global_dim_eff,\n",
    "            'per_example': per_example_dims,\n",
    "            'mean': np.mean(per_example_dims),\n",
    "            'std': np.std(per_example_dims)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {condition}: Global dim_eff = {global_dim_eff}, Mean per-example = {np.mean(per_example_dims):.1f} ± {np.std(per_example_dims):.1f}\")\n",
    "    else:\n",
    "        print(f\"  {condition}: No activations captured\")\n",
    "        dim_eff_results[condition] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE DIM_EFF BY LAYER (for layer-wise analysis)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nComputing layer-wise dimensionality...\")\n",
    "\n",
    "layer_dim_eff = {c: [] for c in conditions}\n",
    "\n",
    "for condition in conditions:\n",
    "    if all_activations[condition]:\n",
    "        stacked = np.stack(all_activations[condition], axis=0)  # (n_examples, n_layers, hidden_dim)\n",
    "        n_examples, n_layers, hidden_dim = stacked.shape\n",
    "        \n",
    "        for layer_idx in range(n_layers):\n",
    "            layer_acts = stacked[:, layer_idx, :]  # (n_examples, hidden_dim)\n",
    "            dim_eff, _ = compute_effective_dimensionality(layer_acts, CONFIG['variance_threshold'])\n",
    "            layer_dim_eff[condition].append(dim_eff)\n",
    "        \n",
    "        print(f\"  {condition}: {layer_dim_eff[condition]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN RECOVERABILITY PROBES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nTraining linear probes for recoverability...\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:  # Not for babble (no correct answers)\n",
    "    if all_activations[condition] and len(all_activations[condition]) >= 10:\n",
    "        # Prepare data\n",
    "        stacked = np.stack(all_activations[condition], axis=0)\n",
    "        n_examples = stacked.shape[0]\n",
    "        X = stacked.reshape(n_examples, -1)  # Flatten layers\n",
    "        y = np.array([results[condition][i].is_correct for i in range(n_examples)])\n",
    "        \n",
    "        # Train probe\n",
    "        mean_acc, std_acc = train_recoverability_probe(X, y, cv_folds=5)\n",
    "        \n",
    "        probe_results[condition] = {\n",
    "            'mean': mean_acc,\n",
    "            'std': std_acc,\n",
    "            'verbalized_accuracy': y.mean()\n",
    "        }\n",
    "        \n",
    "        print(f\"  {condition}:\")\n",
    "        print(f\"    Probe accuracy (Recov): {mean_acc:.3f} ± {std_acc:.3f}\")\n",
    "        print(f\"    Verbalized accuracy:    {y.mean():.3f}\")\n",
    "        print(f\"    Recoverability gap:     {mean_acc - y.mean():+.3f}\")\n",
    "    else:\n",
    "        print(f\"  {condition}: Insufficient data for probe training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SANITY CHECK: INSPECT BASELINE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SANITY CHECK: BASELINE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_results = results['baseline']\n",
    "correct_examples = [r for r in baseline_results if r.is_correct][:2]\n",
    "incorrect_examples = [r for r in baseline_results if not r.is_correct][:5]\n",
    "sample_examples = correct_examples + incorrect_examples\n",
    "\n",
    "for i, r in enumerate(sample_examples):\n",
    "    status = \"✓ CORRECT\" if r.is_correct else \"✗ WRONG\"\n",
    "    print(f\"\\n--- Example {i+1} ({status}) ---\")\n",
    "    print(f\"Question: {r.question[:120]}...\")\n",
    "    print(f\"Ground truth: {r.ground_truth}\")\n",
    "    print(f\"Model output: {r.model_output[:150]}...\")\n",
    "    print(f\"Extracted answer: '{r.extracted_answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SANITY CHECK: INSPECT ENHANCED (CoT) EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SANITY CHECK: ENHANCED (CoT) EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enhanced_results = results['enhanced']\n",
    "correct_cot = [r for r in enhanced_results if r.is_correct][:2]\n",
    "incorrect_cot = [r for r in enhanced_results if not r.is_correct][:2]\n",
    "sample_cot = correct_cot + incorrect_cot\n",
    "\n",
    "for i, r in enumerate(sample_cot):\n",
    "    status = \"✓ CORRECT\" if r.is_correct else \"✗ WRONG\"\n",
    "    print(f\"\\n--- Example {i+1} ({status}) ---\")\n",
    "    print(f\"Question: {r.question[:120]}...\")\n",
    "    print(f\"Ground truth: {r.ground_truth}\")\n",
    "    print(f\"Model output: {r.model_output[:400]}...\")\n",
    "    print(f\"Extracted answer: '{r.extracted_answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SANITY CHECK: INSPECT BABBLE (CONTROL) EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SANITY CHECK: BABBLE (CONTROL) EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "babble_results = results['babble'][:3]\n",
    "\n",
    "for i, r in enumerate(babble_results):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {r.question[:120]}...\")\n",
    "    print(f\"Model output: {r.model_output[:400]}...\")\n",
    "    print(f\"Output length: {r.output_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSTIC: ACTIVATION CAPTURE SUCCESS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ACTIVATION CAPTURE DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for condition in conditions:\n",
    "    n_captured = len(all_activations[condition])\n",
    "    n_total = len(results[condition])\n",
    "    pct = (n_captured / n_total * 100) if n_total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{condition.upper()}:\")\n",
    "    print(f\"  Activations captured: {n_captured}/{n_total} ({pct:.1f}%)\")\n",
    "    \n",
    "    if n_captured > 0:\n",
    "        sample = all_activations[condition][0]\n",
    "        print(f\"  Sample shape: {sample.shape}\")\n",
    "        print(f\"  Sample stats: mean={sample.mean():.4f}, std={sample.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPILE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract metrics into arrays\n",
    "metrics_data = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    metrics_data[condition] = {\n",
    "        'entropy': [r.metrics.entropy for r in results[condition]],\n",
    "        'dim_eff': [r.metrics.dim_eff for r in results[condition]],\n",
    "        'correct': [r.is_correct for r in results[condition]],\n",
    "        'output_length': [r.output_length for r in results[condition]]\n",
    "    }\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for condition in conditions:\n",
    "    print(f\"\\n{condition.upper()}:\")\n",
    "    data = metrics_data[condition]\n",
    "    if condition != 'babble':\n",
    "        print(f\"  Accuracy:       {np.mean(data['correct']):.1%}\")\n",
    "    print(f\"  Entropy:        {np.mean(data['entropy']):.2f} ± {np.std(data['entropy']):.2f}\")\n",
    "    print(f\"  Dim_eff:        {np.mean(data['dim_eff']):.1f} ± {np.std(data['dim_eff']):.1f}\")\n",
    "    print(f\"  Output length:  {np.mean(data['output_length']):.1f} ± {np.std(data['output_length']):.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:  # Not for babble\n",
    "    print(f\"\\n{condition.upper()}:\")\n",
    "    data = metrics_data[condition]\n",
    "    correct = np.array(data['correct']).astype(int)\n",
    "    \n",
    "    # Entropy vs Correctness\n",
    "    try:\n",
    "        r_entropy, p_entropy = stats.pointbiserialr(correct, data['entropy'])\n",
    "        print(f\"  Entropy vs Correct:     r={r_entropy:.3f}, p={p_entropy:.4f}\")\n",
    "    except:\n",
    "        r_entropy, p_entropy = np.nan, np.nan\n",
    "        print(f\"  Entropy vs Correct:     Could not compute\")\n",
    "    \n",
    "    # Dim_eff vs Correctness\n",
    "    try:\n",
    "        dim_eff_vals = [d for d in data['dim_eff'] if d > 0]\n",
    "        if len(dim_eff_vals) > 10:\n",
    "            r_dim, p_dim = stats.pointbiserialr(correct[:len(dim_eff_vals)], dim_eff_vals)\n",
    "            print(f\"  Dim_eff vs Correct:     r={r_dim:.3f}, p={p_dim:.4f}\")\n",
    "        else:\n",
    "            r_dim, p_dim = np.nan, np.nan\n",
    "            print(f\"  Dim_eff vs Correct:     Insufficient data\")\n",
    "    except:\n",
    "        r_dim, p_dim = np.nan, np.nan\n",
    "        print(f\"  Dim_eff vs Correct:     Could not compute\")\n",
    "    \n",
    "    # Output length vs Correctness\n",
    "    try:\n",
    "        r_len, p_len = stats.pointbiserialr(correct, data['output_length'])\n",
    "        print(f\"  Output len vs Correct:  r={r_len:.3f}, p={p_len:.4f}\")\n",
    "    except:\n",
    "        r_len, p_len = np.nan, np.nan\n",
    "        print(f\"  Output len vs Correct:  Could not compute\")\n",
    "    \n",
    "    correlation_results[condition] = {\n",
    "        'entropy_correct': (r_entropy, p_entropy),\n",
    "        'dim_correct': (r_dim, p_dim),\n",
    "        'length_correct': (r_len, p_len)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KEY FINDINGS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "base_acc = np.mean(metrics_data['baseline']['correct'])\n",
    "enh_acc = np.mean(metrics_data['enhanced']['correct'])\n",
    "print(f\"\\n1. ACCURACY:\")\n",
    "print(f\"   Baseline: {base_acc:.1%}\")\n",
    "print(f\"   Enhanced (CoT): {enh_acc:.1%}\")\n",
    "print(f\"   Improvement: {(enh_acc - base_acc)*100:+.1f} percentage points\")\n",
    "\n",
    "# 2. Entropy comparison\n",
    "base_ent = np.mean(metrics_data['baseline']['entropy'])\n",
    "enh_ent = np.mean(metrics_data['enhanced']['entropy'])\n",
    "babble_ent = np.mean(metrics_data['babble']['entropy'])\n",
    "print(f\"\\n2. INTENTION ENTROPY (lower = more decided):\")\n",
    "print(f\"   Baseline: {base_ent:.2f}\")\n",
    "print(f\"   Enhanced (CoT): {enh_ent:.2f}\")\n",
    "print(f\"   Babble (control): {babble_ent:.2f}\")\n",
    "print(f\"   → CoT reduces entropy by {(1 - enh_ent/base_ent)*100:.1f}%\")\n",
    "\n",
    "# 3. Dim_eff comparison (LENGTH BIAS TEST)\n",
    "base_dim = np.mean([d for d in metrics_data['baseline']['dim_eff'] if d > 0] or [0])\n",
    "enh_dim = np.mean([d for d in metrics_data['enhanced']['dim_eff'] if d > 0] or [0])\n",
    "babble_dim = np.mean([d for d in metrics_data['babble']['dim_eff'] if d > 0] or [0])\n",
    "print(f\"\\n3. EFFECTIVE DIMENSIONALITY (higher = richer intention):\")\n",
    "print(f\"   Baseline: {base_dim:.1f}\")\n",
    "print(f\"   Enhanced (CoT): {enh_dim:.1f}\")\n",
    "print(f\"   Babble (control): {babble_dim:.1f}\")\n",
    "if enh_dim > 0 and babble_dim > 0:\n",
    "    if enh_dim > babble_dim:\n",
    "        print(f\"   → dim_eff(CoT) > dim_eff(Babble): LENGTH BIAS RULED OUT ✓\")\n",
    "    else:\n",
    "        print(f\"   → dim_eff(CoT) ≤ dim_eff(Babble): Possible length bias ⚠️\")\n",
    "\n",
    "# 4. Output length comparison\n",
    "base_len = np.mean(metrics_data['baseline']['output_length'])\n",
    "enh_len = np.mean(metrics_data['enhanced']['output_length'])\n",
    "babble_len = np.mean(metrics_data['babble']['output_length'])\n",
    "print(f\"\\n4. OUTPUT LENGTH (words):\")\n",
    "print(f\"   Baseline: {base_len:.1f}\")\n",
    "print(f\"   Enhanced (CoT): {enh_len:.1f}\")\n",
    "print(f\"   Babble (control): {babble_len:.1f}\")\n",
    "\n",
    "# 5. Recoverability\n",
    "print(f\"\\n5. RECOVERABILITY (probe accuracy vs verbalized):\")\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    if condition in probe_results:\n",
    "        probe_acc = probe_results[condition]['mean']\n",
    "        verb_acc = probe_results[condition]['verbalized_accuracy']\n",
    "        gap = probe_acc - verb_acc\n",
    "        print(f\"   {condition}: Probe={probe_acc:.3f}, Verbalized={verb_acc:.3f}, Gap={gap:+.3f}\")\n",
    "    else:\n",
    "        print(f\"   {condition}: Not computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION SETUP\n",
    "# =============================================================================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.titlesize': 13,\n",
    "})\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "COLORS = {\n",
    "    'baseline': '#1f77b4',\n",
    "    'enhanced': '#ff7f0e',\n",
    "    'babble': '#9467bd',\n",
    "    'correct': '#2ca02c',\n",
    "    'incorrect': '#d62728',\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "print(\"✓ Visualization setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 1: MAIN COMPARISON (Accuracy, Entropy, Dim_eff)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# (A) Accuracy comparison\n",
    "ax = axes[0]\n",
    "conds = ['Baseline', 'CoT']\n",
    "accs = [base_acc, enh_acc]\n",
    "colors = [COLORS['baseline'], COLORS['enhanced']]\n",
    "bars = ax.bar(conds, accs, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('(A) Task Accuracy')\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "           f'{acc:.1%}', ha='center', fontsize=10)\n",
    "\n",
    "# (B) Entropy comparison (all 3 conditions)\n",
    "ax = axes[1]\n",
    "conds = ['Baseline', 'CoT', 'Babble']\n",
    "ents = [base_ent, enh_ent, babble_ent]\n",
    "colors = [COLORS['baseline'], COLORS['enhanced'], COLORS['babble']]\n",
    "bars = ax.bar(conds, ents, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Intention Entropy (bits)')\n",
    "ax.set_title('(B) Intention Entropy $H_{int}(I)$')\n",
    "for bar, ent in zip(bars, ents):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "           f'{ent:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "# (C) Dim_eff comparison (all 3 conditions)\n",
    "ax = axes[2]\n",
    "dims = [base_dim, enh_dim, babble_dim]\n",
    "bars = ax.bar(conds, dims, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Effective Dimensionality')\n",
    "ax.set_title('(C) Intention Richness $dim_{eff}(I)$')\n",
    "for bar, dim in zip(bars, dims):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "           f'{dim:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig1_main_comparison.pdf\", bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig1_main_comparison.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 2: LAYER-WISE DIM_EFF (Gemini's suggestion)\n",
    "# =============================================================================\n",
    "\n",
    "if any(layer_dim_eff[c] for c in conditions):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x = np.arange(len(CONFIG['extraction_layers']))\n",
    "    \n",
    "    for condition in conditions:\n",
    "        if layer_dim_eff[condition]:\n",
    "            ax.plot(x, layer_dim_eff[condition], 'o-', \n",
    "                   color=COLORS[condition], \n",
    "                   label=condition.title(),\n",
    "                   linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"L{l}\" for l in CONFIG['extraction_layers']])\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Effective Dimensionality ($dim_{eff}$)')\n",
    "    ax.set_title('Intention Richness Across Layers')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if CONFIG['save_figures']:\n",
    "        plt.savefig(f\"{CONFIG['output_dir']}/fig2_layer_dim_eff.pdf\", bbox_inches='tight', dpi=300)\n",
    "        plt.savefig(f\"{CONFIG['output_dir']}/fig2_layer_dim_eff.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No layer-wise dim_eff data available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 3: ENTROPY DISTRIBUTION BY CORRECTNESS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for idx, condition in enumerate(['baseline', 'enhanced']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    entropy_vals = metrics_data[condition]['entropy']\n",
    "    correct_vals = metrics_data[condition]['correct']\n",
    "    \n",
    "    entropy_correct = [e for e, c in zip(entropy_vals, correct_vals) if c]\n",
    "    entropy_incorrect = [e for e, c in zip(entropy_vals, correct_vals) if not c]\n",
    "    \n",
    "    data = [entropy_incorrect, entropy_correct]\n",
    "    labels = ['Incorrect', 'Correct']\n",
    "    colors_box = [COLORS['incorrect'], COLORS['correct']]\n",
    "    \n",
    "    bp = ax.boxplot(data, patch_artist=True, labels=labels)\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel('Intention Entropy (bits)')\n",
    "    ax.set_title(f'{condition.title()}')\n",
    "    \n",
    "    # Add correlation info\n",
    "    if condition in correlation_results:\n",
    "        r, p = correlation_results[condition]['entropy_correct']\n",
    "        if not np.isnan(r):\n",
    "            ax.text(0.95, 0.95, f'r = {r:.3f}\\np = {p:.3f}',\n",
    "                   transform=ax.transAxes, ha='right', va='top',\n",
    "                   fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Entropy Distribution by Answer Correctness', fontsize=12)\n",
    "plt.tight_layout()\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig3_entropy_correctness.pdf\", bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig3_entropy_correctness.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 4: LENGTH BIAS CONTROL (Critical for reviewers)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# (A) Output length comparison\n",
    "ax = axes[0]\n",
    "conds = ['Baseline', 'CoT', 'Babble']\n",
    "lengths = [base_len, enh_len, babble_len]\n",
    "colors = [COLORS['baseline'], COLORS['enhanced'], COLORS['babble']]\n",
    "bars = ax.bar(conds, lengths, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Output Length (words)')\n",
    "ax.set_title('(A) Output Length by Condition')\n",
    "for bar, length in zip(bars, lengths):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "           f'{length:.0f}', ha='center', fontsize=10)\n",
    "\n",
    "# (B) Dim_eff vs Length scatter\n",
    "ax = axes[1]\n",
    "for condition in conditions:\n",
    "    dims = [d for d in metrics_data[condition]['dim_eff'] if d > 0]\n",
    "    lens = [metrics_data[condition]['output_length'][i] \n",
    "           for i, d in enumerate(metrics_data[condition]['dim_eff']) if d > 0]\n",
    "    if dims and lens:\n",
    "        ax.scatter(lens, dims, alpha=0.5, color=COLORS[condition], \n",
    "                  label=condition.title(), s=30)\n",
    "\n",
    "ax.set_xlabel('Output Length (words)')\n",
    "ax.set_ylabel('Effective Dimensionality')\n",
    "ax.set_title('(B) Dim_eff vs Output Length')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig4_length_bias_control.pdf\", bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig4_length_bias_control.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 5: RECOVERABILITY GAP\n",
    "# =============================================================================\n",
    "\n",
    "if probe_results:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    x = np.arange(len(probe_results))\n",
    "    width = 0.35\n",
    "    \n",
    "    conditions_with_probes = list(probe_results.keys())\n",
    "    probe_accs = [probe_results[c]['mean'] for c in conditions_with_probes]\n",
    "    probe_stds = [probe_results[c]['std'] for c in conditions_with_probes]\n",
    "    verb_accs = [probe_results[c]['verbalized_accuracy'] for c in conditions_with_probes]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, probe_accs, width, label='Probe on I (Recoverability)',\n",
    "                   color=COLORS['enhanced'], alpha=0.8, yerr=probe_stds, capsize=5)\n",
    "    bars2 = ax.bar(x + width/2, verb_accs, width, label='Verbalized Output',\n",
    "                   color=COLORS['baseline'], alpha=0.8)\n",
    "    \n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([c.title() for c in conditions_with_probes])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title('Information Recovery: Probe vs. Verbalized Output')\n",
    "    \n",
    "    # Add gap annotations\n",
    "    for i, (p, v) in enumerate(zip(probe_accs, verb_accs)):\n",
    "        gap = p - v\n",
    "        mid_y = (p + v) / 2\n",
    "        ax.annotate(f'Gap: {gap:+.1%}', xy=(i, mid_y), fontsize=9,\n",
    "                   ha='center', va='center',\n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if CONFIG['save_figures']:\n",
    "        plt.savefig(f\"{CONFIG['output_dir']}/fig5_recoverability_gap.pdf\", bbox_inches='tight', dpi=300)\n",
    "        plt.savefig(f\"{CONFIG['output_dir']}/fig5_recoverability_gap.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Probe results not available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TABLE\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Build summary table\n",
    "rows = []\n",
    "rows.append(['Accuracy', f\"{base_acc:.1%}\", f\"{enh_acc:.1%}\", \"N/A\"])\n",
    "rows.append(['Entropy (mean±std)', \n",
    "             f\"{base_ent:.2f}±{np.std(metrics_data['baseline']['entropy']):.2f}\",\n",
    "             f\"{enh_ent:.2f}±{np.std(metrics_data['enhanced']['entropy']):.2f}\",\n",
    "             f\"{babble_ent:.2f}±{np.std(metrics_data['babble']['entropy']):.2f}\"])\n",
    "rows.append(['Dim_eff (mean±std)',\n",
    "             f\"{base_dim:.1f}±{np.std([d for d in metrics_data['baseline']['dim_eff'] if d > 0] or [0]):.1f}\",\n",
    "             f\"{enh_dim:.1f}±{np.std([d for d in metrics_data['enhanced']['dim_eff'] if d > 0] or [0]):.1f}\",\n",
    "             f\"{babble_dim:.1f}±{np.std([d for d in metrics_data['babble']['dim_eff'] if d > 0] or [0]):.1f}\"])\n",
    "rows.append(['Output Length',\n",
    "             f\"{base_len:.1f}\",\n",
    "             f\"{enh_len:.1f}\",\n",
    "             f\"{babble_len:.1f}\"])\n",
    "\n",
    "if 'baseline' in probe_results:\n",
    "    rows.append(['Probe Accuracy',\n",
    "                 f\"{probe_results['baseline']['mean']:.3f}±{probe_results['baseline']['std']:.3f}\",\n",
    "                 f\"{probe_results['enhanced']['mean']:.3f}±{probe_results['enhanced']['std']:.3f}\" if 'enhanced' in probe_results else \"N/A\",\n",
    "                 \"N/A\"])\n",
    "\n",
    "summary_table = pd.DataFrame(rows, columns=['Metric', 'Baseline', 'CoT', 'Babble'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS TABLE (for paper)\")\n",
    "print(\"=\"*60)\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_table.to_csv(f\"{CONFIG['output_dir']}/results_summary.csv\", index=False)\n",
    "print(f\"\\n✓ Results saved to {CONFIG['output_dir']}/results_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT LATEX TABLE\n",
    "# =============================================================================\n",
    "\n",
    "latex_lines = [\n",
    "    r\"\\begin{table}[h]\",\n",
    "    r\"\\centering\",\n",
    "    r\"\\caption{Experiment 4.1 Results: Intention Metrics vs. Reasoning Accuracy}\",\n",
    "    r\"\\label{tab:exp41_results}\",\n",
    "    r\"\\begin{tabular}{lccc}\",\n",
    "    r\"\\hline\",\n",
    "    r\"\\textbf{Metric} & \\textbf{Baseline} & \\textbf{CoT} & \\textbf{Babble} \\\\\",\n",
    "    r\"\\hline\",\n",
    "    f\"Accuracy & {base_acc:.1%} & {enh_acc:.1%} & N/A \\\\\\\\\",\n",
    "    f\"$H_{{int}}(I)$ & {base_ent:.2f} & {enh_ent:.2f} & {babble_ent:.2f} \\\\\\\\\",\n",
    "    f\"$dim_{{eff}}(I)$ & {base_dim:.1f} & {enh_dim:.1f} & {babble_dim:.1f} \\\\\\\\\",\n",
    "    f\"Output Length & {base_len:.0f} & {enh_len:.0f} & {babble_len:.0f} \\\\\\\\\",\n",
    "]\n",
    "\n",
    "if probe_results:\n",
    "    base_probe = probe_results.get('baseline', {}).get('mean', 0)\n",
    "    enh_probe = probe_results.get('enhanced', {}).get('mean', 0)\n",
    "    latex_lines.append(f\"Probe Accuracy & {base_probe:.3f} & {enh_probe:.3f} & N/A \\\\\\\\\")\n",
    "\n",
    "latex_lines.extend([\n",
    "    r\"\\hline\",\n",
    "    r\"\\end{tabular}\",\n",
    "    r\"\\end{table}\"\n",
    "])\n",
    "\n",
    "latex_table = \"\\n\".join(latex_lines)\n",
    "\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_table)\n",
    "\n",
    "with open(f\"{CONFIG['output_dir']}/results_table.tex\", 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"\\n✓ LaTeX table saved to {CONFIG['output_dir']}/results_table.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE FULL RESULTS AS JSON\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "full_results = {\n",
    "    'config': CONFIG,\n",
    "    'summary': {\n",
    "        'baseline_accuracy': float(base_acc),\n",
    "        'enhanced_accuracy': float(enh_acc),\n",
    "        'baseline_entropy': float(base_ent),\n",
    "        'enhanced_entropy': float(enh_ent),\n",
    "        'babble_entropy': float(babble_ent),\n",
    "        'baseline_dim_eff': float(base_dim),\n",
    "        'enhanced_dim_eff': float(enh_dim),\n",
    "        'babble_dim_eff': float(babble_dim),\n",
    "    },\n",
    "    'layer_dim_eff': {k: [int(v) for v in vals] for k, vals in layer_dim_eff.items() if vals},\n",
    "    'probe_results': probe_results if probe_results else {},\n",
    "    'n_problems': len(problems),\n",
    "}\n",
    "\n",
    "with open(f\"{CONFIG['output_dir']}/full_results.json\", 'w') as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Full results saved to {CONFIG['output_dir']}/full_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIST ALL GENERATED FILES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for f in sorted(os.listdir(CONFIG['output_dir'])):\n",
    "    filepath = os.path.join(CONFIG['output_dir'], f)\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(f\"  {f} ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "This experiment tested three key predictions of the Intention Collapse framework:\n",
    "\n",
    "1. **Intention Entropy**: Lower entropy indicates a more \"decided\" intention state\n",
    "2. **Effective Dimensionality**: Higher dim_eff correlates with richer reasoning\n",
    "3. **Recoverability**: Pre-collapse states contain more information than verbalized outputs\n",
    "\n",
    "### Methodological Strengths\n",
    "\n",
    "- **Babble control condition** rules out length bias in dim_eff measurements\n",
    "- **Layer-wise analysis** shows where information is encoded\n",
    "- **Sanity checks** verify answer extraction is working correctly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [ ] Increase N to 500+ for statistical power\n",
    "- [ ] Try other models (Llama-3-8B, Qwen-2-7B)\n",
    "- [ ] Implement Experiments 4.2 and 4.3\n",
    "- [ ] Add statistical significance tests (paired t-tests, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
