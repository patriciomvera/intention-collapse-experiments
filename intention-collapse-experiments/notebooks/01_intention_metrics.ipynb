{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intention Collapse: Experiment 4.1\n",
    "## Correlating Intention Metrics with Reasoning Accuracy\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/intention-collapse-experiments/blob/main/notebooks/01_intention_metrics.ipynb)\n",
    "\n",
    "This notebook implements the experimental protocol from Section 4.1 of the Intention Collapse paper:\n",
    "\n",
    "> \"To assess whether richer intention states predict better performance, we propose measuring our metrics across varying reasoning regimes and correlating them with final accuracy.\"\n",
    "\n",
    "### Metrics Implemented\n",
    "1. **Intention Entropy** $H_{int}(I)$: Shannon entropy of next-token distribution\n",
    "2. **Effective Dimensionality** $dim_{eff}(I)$: PCA-based dimensionality of hidden activations\n",
    "3. **Latent Recoverability** $Recov(I; Z)$: Linear probe accuracy for predicting correctness\n",
    "\n",
    "### Experimental Conditions\n",
    "- **Baseline**: Zero-shot (direct answer)\n",
    "- **Enhanced**: Chain-of-thought reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (this may take a few minutes)\n",
    "!pip install -q torch transformers accelerate bitsandbytes\n",
    "!pip install -q datasets scikit-learn scipy\n",
    "!pip install -q matplotlib seaborn tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Please enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hugging Face token\n",
    "# Option 1: Use Colab Secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"✓ Loaded HF_TOKEN from Colab Secrets\")\n",
    "except:\n",
    "    # Option 2: Manual input\n",
    "    import getpass\n",
    "    HF_TOKEN = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "    print(\"✓ Token entered manually\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up experiment parameters. Adjust these based on your available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model_name': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    'quantization': '4bit',  # Options: '4bit', '8bit', 'none'\n",
    "    'extraction_layers': [27, 28, 29, 30, 31],  # Last 5 layers for Mistral-7B\n",
    "    \n",
    "    # Dataset settings\n",
    "    'dataset': 'gsm8k',\n",
    "    'subset_size': 200,  # Reduce for faster testing, increase for publication\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Generation settings\n",
    "    'max_new_tokens_baseline': 50,\n",
    "    'max_new_tokens_cot': 512,\n",
    "    'temperature': 0.0,  # Greedy decoding\n",
    "    \n",
    "    # Metric settings\n",
    "    'variance_threshold': 0.90,  # For dim_eff\n",
    "    'entropy_top_k': 100,        # For H_int\n",
    "    'probe_regularization': 1.0, # For Recov\n",
    "    \n",
    "    # Output settings\n",
    "    'output_dir': 'results',\n",
    "    'save_figures': True,\n",
    "}\n",
    "\n",
    "# Prompt templates\n",
    "PROMPTS = {\n",
    "    'baseline': \"\"\"Solve this math problem. Give only the final numerical answer.\n",
    "\n",
    "Problem: {question}\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    'enhanced': \"\"\"Solve this math problem step by step. Show your reasoning, then give the final answer.\n",
    "\n",
    "Problem: {question}\n",
    "Solution:\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Dataset: {CONFIG['dataset']} ({CONFIG['subset_size']} problems)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Implementation\n",
    "\n",
    "Define the classes and functions needed for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MathProblem:\n",
    "    \"\"\"Container for a math problem from GSM8K.\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    final_answer: str\n",
    "    idx: int\n",
    "\n",
    "@dataclass\n",
    "class IntentionMetrics:\n",
    "    \"\"\"Container for intention metrics for a single example.\"\"\"\n",
    "    entropy: float\n",
    "    dim_eff: int\n",
    "    recoverability: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Results from a single problem evaluation.\"\"\"\n",
    "    problem_idx: int\n",
    "    condition: str\n",
    "    question: str\n",
    "    ground_truth: str\n",
    "    model_output: str\n",
    "    extracted_answer: str\n",
    "    is_correct: bool\n",
    "    metrics: IntentionMetrics\n",
    "    activations: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ACTIVATION EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"\n",
    "    Extract hidden state activations from specified transformer layers.\n",
    "    \n",
    "    Reference: Section 2.1 \"Intention State in Practice\"\n",
    "        I_t = (h_t, KV_t, R_t, c_t) ∈ I\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layers: List[int]):\n",
    "        self.model = model\n",
    "        self.layers = sorted(layers)\n",
    "        self._hooks = []\n",
    "        self._activations = {l: [] for l in layers}\n",
    "        self._is_capturing = False\n",
    "    \n",
    "    def _get_layer_module(self, layer_idx: int):\n",
    "        \"\"\"Get the module for a specific layer.\"\"\"\n",
    "        return self.model.model.layers[layer_idx]\n",
    "    \n",
    "    def _create_hook(self, layer_idx: int):\n",
    "        \"\"\"Create a hook function for a specific layer.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if not self._is_capturing:\n",
    "                return\n",
    "            hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "            self._activations[layer_idx].append(hidden_states.detach().cpu())\n",
    "        return hook\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks on specified layers.\"\"\"\n",
    "        for layer_idx in self.layers:\n",
    "            layer_module = self._get_layer_module(layer_idx)\n",
    "            hook = layer_module.register_forward_hook(self._create_hook(layer_idx))\n",
    "            self._hooks.append(hook)\n",
    "    \n",
    "    def _remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self._hooks:\n",
    "            hook.remove()\n",
    "        self._hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self._activations = {l: [] for l in self.layers}\n",
    "    \n",
    "    @contextmanager\n",
    "    def capture(self):\n",
    "        \"\"\"Context manager for capturing activations.\"\"\"\n",
    "        self.clear()\n",
    "        self._register_hooks()\n",
    "        self._is_capturing = True\n",
    "        try:\n",
    "            yield self\n",
    "        finally:\n",
    "            self._is_capturing = False\n",
    "            self._remove_hooks()\n",
    "    \n",
    "    def get_activations(self, aggregate: str = \"last\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get extracted activations.\n",
    "        \n",
    "        Args:\n",
    "            aggregate: \"last\" for last token, \"mean\" for mean across positions\n",
    "            \n",
    "        Returns:\n",
    "            Array of shape (n_layers, hidden_dim)\n",
    "        \"\"\"\n",
    "        all_activations = []\n",
    "        for l in self.layers:\n",
    "            if not self._activations[l]:\n",
    "                raise ValueError(f\"No activations captured for layer {l}\")\n",
    "            layer_acts = torch.cat(self._activations[l], dim=0)\n",
    "            if aggregate == \"last\":\n",
    "                layer_acts = layer_acts[:, -1, :]  # Last position\n",
    "            elif aggregate == \"mean\":\n",
    "                layer_acts = layer_acts.mean(dim=1)\n",
    "            all_activations.append(layer_acts.numpy())\n",
    "        return np.stack(all_activations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTENTION METRICS IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_intention_entropy(logits: torch.Tensor, top_k: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Compute intention entropy H_int(I) from logits.\n",
    "    \n",
    "    Reference: Section 2.2\n",
    "        H_int(I) ≜ H[p_θ(y_1 | I, x)]\n",
    "        \"Lower entropy indicates a more decided intention.\"\n",
    "    \"\"\"\n",
    "    if logits.dim() == 2:\n",
    "        logits = logits[-1]\n",
    "    \n",
    "    if top_k > 0 and top_k < logits.size(-1):\n",
    "        top_logits, _ = torch.topk(logits, top_k)\n",
    "        probs = F.softmax(top_logits, dim=-1)\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    eps = 1e-10\n",
    "    log_probs = torch.log2(probs + eps)\n",
    "    entropy = -torch.sum(probs * log_probs).item()\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def compute_effective_dimensionality(\n",
    "    activations: np.ndarray,\n",
    "    variance_threshold: float = 0.90\n",
    ") -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute effective intention dimensionality dim_eff(I) using PCA.\n",
    "    \n",
    "    Reference: Section 2.2\n",
    "        \"The effective dimensionality is the smallest k such that\n",
    "         Σᵢ₌₁ᵏ λᵢ / Σⱼ λⱼ ≥ 0.9\"\n",
    "    \"\"\"\n",
    "    if activations.ndim == 3:\n",
    "        n_layers, n_samples, hidden_dim = activations.shape\n",
    "        activations = activations.reshape(n_layers * n_samples, hidden_dim)\n",
    "    elif activations.ndim == 2:\n",
    "        pass  # Already (n_samples, hidden_dim)\n",
    "    else:\n",
    "        activations = activations.flatten().reshape(1, -1)\n",
    "    \n",
    "    n_samples, n_features = activations.shape\n",
    "    n_components = min(n_samples, n_features)\n",
    "    \n",
    "    if n_components < 2:\n",
    "        return 1, np.array([1.0])\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(activations)\n",
    "    \n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    dim_eff = np.searchsorted(cumulative_variance, variance_threshold) + 1\n",
    "    dim_eff = min(dim_eff, n_components)\n",
    "    \n",
    "    return dim_eff, pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "def train_recoverability_probe(\n",
    "    activations: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    cv_folds: int = 5\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train a linear probe to measure latent recoverability Recov(I; Z).\n",
    "    \n",
    "    Reference: Section 2.2\n",
    "        \"Train linear or shallow probes on frozen I to predict downstream\n",
    "         variables Z that are known to the model but not necessarily verbalized.\"\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (mean_cv_accuracy, std_cv_accuracy)\n",
    "    \"\"\"\n",
    "    if activations.ndim == 3:\n",
    "        n_layers, n_samples, hidden_dim = activations.shape\n",
    "        activations = activations.transpose(1, 0, 2).reshape(n_samples, -1)\n",
    "    \n",
    "    labels = np.asarray(labels).astype(int)\n",
    "    \n",
    "    # Check if we have enough samples and class balance\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return 0.5, 0.0  # Can't train with single class\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(\n",
    "            C=1.0/CONFIG['probe_regularization'],\n",
    "            max_iter=1000,\n",
    "            random_state=CONFIG['seed']\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        cv_scores = cross_val_score(pipeline, activations, labels, cv=cv_folds)\n",
    "        return cv_scores.mean(), cv_scores.std()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Probe training failed: {e}\")\n",
    "        return 0.5, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def load_gsm8k_problems(subset_size: int, seed: int) -> List[MathProblem]:\n",
    "    \"\"\"Load GSM8K dataset.\"\"\"\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "    \n",
    "    if subset_size < len(dataset):\n",
    "        random.seed(seed)\n",
    "        indices = random.sample(range(len(dataset)), subset_size)\n",
    "        dataset = dataset.select(indices)\n",
    "    \n",
    "    problems = []\n",
    "    for idx, item in enumerate(dataset):\n",
    "        solution = item['answer']\n",
    "        # Extract final answer (GSM8K format: \"#### <answer>\")\n",
    "        match = re.search(r'####\\s*(.+?)$', solution, re.MULTILINE)\n",
    "        final_answer = match.group(1).strip().replace(',', '') if match else \"\"\n",
    "        \n",
    "        problems.append(MathProblem(\n",
    "            question=item['question'],\n",
    "            answer=solution,\n",
    "            final_answer=final_answer,\n",
    "            idx=idx\n",
    "        ))\n",
    "    \n",
    "    return problems\n",
    "\n",
    "\n",
    "def extract_answer(model_output: str) -> str:\n",
    "    \"\"\"Extract numerical answer from model output.\"\"\"\n",
    "    output = model_output.strip()\n",
    "    \n",
    "    # Try various patterns\n",
    "    patterns = [\n",
    "        r'####\\s*(-?\\d+\\.?\\d*)',\n",
    "        r'[Aa]nswer[:\\s]+(-?\\d+\\.?\\d*)',\n",
    "        r'[Tt]he answer is[:\\s]+(-?\\d+\\.?\\d*)',\n",
    "        r'=\\s*(-?\\d+\\.?\\d*)\\s*$',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, output)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Fallback: last number in output\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', output)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_answer(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Check if predicted answer matches ground truth.\"\"\"\n",
    "    pred_clean = predicted.strip().replace(',', '').replace('$', '')\n",
    "    truth_clean = ground_truth.strip().replace(',', '').replace('$', '')\n",
    "    \n",
    "    if pred_clean == truth_clean:\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        pred_num = float(pred_clean)\n",
    "        truth_num = float(truth_clean)\n",
    "        return abs(pred_num - truth_num) < 1e-6\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Data\n",
    "\n",
    "Load the language model with 4-bit quantization and the GSM8K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "if CONFIG['quantization'] == '4bit':\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "elif CONFIG['quantization'] == '8bit':\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "else:\n",
    "    quantization_config = None\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Loading GSM8K dataset ({CONFIG['subset_size']} problems)...\")\n",
    "\n",
    "problems = load_gsm8k_problems(\n",
    "    subset_size=CONFIG['subset_size'],\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(problems)} problems\")\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"  Question: {problems[0].question[:200]}...\")\n",
    "print(f\"  Answer: {problems[0].final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Experiment\n",
    "\n",
    "Evaluate the model under both baseline (zero-shot) and enhanced (chain-of-thought) conditions, extracting intention metrics throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT RUNNER\n",
    "# =============================================================================\n",
    "\n",
    "def run_single_problem(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    extractor: ActivationExtractor,\n",
    "    problem: MathProblem,\n",
    "    condition: str,\n",
    "    prompt_template: str,\n",
    "    max_new_tokens: int\n",
    ") -> ExperimentResult:\n",
    "    \"\"\"\n",
    "    Run model on a single problem and extract metrics.\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = prompt_template.format(question=problem.question)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with activation capture\n",
    "    with torch.no_grad():\n",
    "        with extractor.capture():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=CONFIG['temperature'] if CONFIG['temperature'] > 0 else None,\n",
    "                do_sample=CONFIG['temperature'] > 0,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
    "    model_output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract activations\n",
    "    try:\n",
    "        activations = extractor.get_activations(aggregate=\"last\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to get activations: {e}\")\n",
    "        activations = None\n",
    "    \n",
    "    # Compute entropy from the first generated token's logits\n",
    "    if outputs.scores:\n",
    "        first_logits = outputs.scores[0][0]  # First token logits\n",
    "        entropy = compute_intention_entropy(first_logits, top_k=CONFIG['entropy_top_k'])\n",
    "    else:\n",
    "        entropy = 0.0\n",
    "    \n",
    "    # Compute dim_eff (will be done batch-wise later for proper PCA)\n",
    "    dim_eff = 0  # Placeholder, computed after collecting all activations\n",
    "    \n",
    "    # Evaluate answer\n",
    "    extracted_answer = extract_answer(model_output)\n",
    "    is_correct = evaluate_answer(extracted_answer, problem.final_answer)\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_idx=problem.idx,\n",
    "        condition=condition,\n",
    "        question=problem.question,\n",
    "        ground_truth=problem.final_answer,\n",
    "        model_output=model_output,\n",
    "        extracted_answer=extracted_answer,\n",
    "        is_correct=is_correct,\n",
    "        metrics=IntentionMetrics(entropy=entropy, dim_eff=dim_eff),\n",
    "        activations=activations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN MAIN EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = ActivationExtractor(model, CONFIG['extraction_layers'])\n",
    "\n",
    "# Storage for results\n",
    "results = {'baseline': [], 'enhanced': []}\n",
    "all_activations = {'baseline': [], 'enhanced': []}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING EXPERIMENT 4.1: Intention Metrics vs. Accuracy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    print(f\"\\n--- Condition: {condition.upper()} ---\")\n",
    "    \n",
    "    prompt_template = PROMPTS[condition]\n",
    "    max_tokens = (\n",
    "        CONFIG['max_new_tokens_baseline'] \n",
    "        if condition == 'baseline' \n",
    "        else CONFIG['max_new_tokens_cot']\n",
    "    )\n",
    "    \n",
    "    for problem in tqdm(problems, desc=f\"Running {condition}\"):\n",
    "        result = run_single_problem(\n",
    "            model, tokenizer, extractor,\n",
    "            problem, condition,\n",
    "            prompt_template, max_tokens\n",
    "        )\n",
    "        results[condition].append(result)\n",
    "        \n",
    "        if result.activations is not None:\n",
    "            all_activations[condition].append(result.activations)\n",
    "        \n",
    "        # Clear CUDA cache periodically\n",
    "        if problem.idx % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = sum(r.is_correct for r in results[condition]) / len(results[condition])\n",
    "    print(f\"  Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE DIM_EFF (requires all activations)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nComputing effective dimensionality across all examples...\")\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    if all_activations[condition]:\n",
    "        # Stack all activations: (n_examples, n_layers, hidden_dim)\n",
    "        stacked = np.stack(all_activations[condition], axis=0)\n",
    "        # Reshape to (n_examples * n_layers, hidden_dim) for PCA\n",
    "        n_examples, n_layers, hidden_dim = stacked.shape\n",
    "        flattened = stacked.reshape(n_examples * n_layers, hidden_dim)\n",
    "        \n",
    "        global_dim_eff, explained_var = compute_effective_dimensionality(\n",
    "            flattened,\n",
    "            variance_threshold=CONFIG['variance_threshold']\n",
    "        )\n",
    "        \n",
    "        # Update individual results with per-example dim_eff\n",
    "        for i, result in enumerate(results[condition]):\n",
    "            if result.activations is not None:\n",
    "                per_example_acts = result.activations.reshape(\n",
    "                    result.activations.shape[0], -1\n",
    "                )\n",
    "                dim_eff, _ = compute_effective_dimensionality(\n",
    "                    per_example_acts,\n",
    "                    variance_threshold=CONFIG['variance_threshold']\n",
    "                )\n",
    "                result.metrics.dim_eff = dim_eff\n",
    "        \n",
    "        print(f\"  {condition}: Global dim_eff = {global_dim_eff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN RECOVERABILITY PROBES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nTraining linear probes for recoverability...\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    if all_activations[condition]:\n",
    "        # Prepare data\n",
    "        stacked = np.stack(all_activations[condition], axis=0)\n",
    "        n_examples, n_layers, hidden_dim = stacked.shape\n",
    "        X = stacked.reshape(n_examples, -1)  # Flatten layers\n",
    "        y = np.array([r.is_correct for r in results[condition]])\n",
    "        \n",
    "        # Train probe\n",
    "        mean_acc, std_acc = train_recoverability_probe(X, y, cv_folds=5)\n",
    "        \n",
    "        probe_results[condition] = {\n",
    "            'mean': mean_acc,\n",
    "            'std': std_acc,\n",
    "            'verbalized_accuracy': y.mean()\n",
    "        }\n",
    "        \n",
    "        print(f\"  {condition}:\")\n",
    "        print(f\"    Probe accuracy (Recov): {mean_acc:.3f} ± {std_acc:.3f}\")\n",
    "        print(f\"    Verbalized accuracy:    {y.mean():.3f}\")\n",
    "        print(f\"    Recoverability gap:     {mean_acc - y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results\n",
    "\n",
    "Compute statistics and correlations between intention metrics and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPILE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract metrics into arrays\n",
    "metrics_data = {}\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    metrics_data[condition] = {\n",
    "        'entropy': [r.metrics.entropy for r in results[condition]],\n",
    "        'dim_eff': [r.metrics.dim_eff for r in results[condition]],\n",
    "        'correct': [r.is_correct for r in results[condition]],\n",
    "        'output_length': [len(r.model_output.split()) for r in results[condition]]\n",
    "    }\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    print(f\"\\n{condition.upper()}:\")\n",
    "    data = metrics_data[condition]\n",
    "    print(f\"  Accuracy:       {np.mean(data['correct']):.1%}\")\n",
    "    print(f\"  Entropy:        {np.mean(data['entropy']):.2f} ± {np.std(data['entropy']):.2f}\")\n",
    "    print(f\"  Dim_eff:        {np.mean(data['dim_eff']):.1f} ± {np.std(data['dim_eff']):.1f}\")\n",
    "    print(f\"  Output length:  {np.mean(data['output_length']):.1f} ± {np.std(data['output_length']):.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE CORRELATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    print(f\"\\n{condition.upper()}:\")\n",
    "    data = metrics_data[condition]\n",
    "    correct = np.array(data['correct']).astype(int)\n",
    "    \n",
    "    # Correlation: Entropy vs Correctness\n",
    "    r_entropy, p_entropy = stats.pointbiserialr(correct, data['entropy'])\n",
    "    print(f\"  Entropy vs Correct:     r={r_entropy:.3f}, p={p_entropy:.4f}\")\n",
    "    \n",
    "    # Correlation: Dim_eff vs Correctness  \n",
    "    r_dim, p_dim = stats.pointbiserialr(correct, data['dim_eff'])\n",
    "    print(f\"  Dim_eff vs Correct:     r={r_dim:.3f}, p={p_dim:.4f}\")\n",
    "    \n",
    "    # Correlation: Output length vs Correctness\n",
    "    r_len, p_len = stats.pointbiserialr(correct, data['output_length'])\n",
    "    print(f\"  Output len vs Correct:  r={r_len:.3f}, p={p_len:.4f}\")\n",
    "    \n",
    "    correlation_results[condition] = {\n",
    "        'entropy_correct': (r_entropy, p_entropy),\n",
    "        'dim_correct': (r_dim, p_dim),\n",
    "        'length_correct': (r_len, p_len)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KEY FINDINGS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hypothesis 1: Enhanced condition should have higher dim_eff\n",
    "base_dim = np.mean(metrics_data['baseline']['dim_eff'])\n",
    "enh_dim = np.mean(metrics_data['enhanced']['dim_eff'])\n",
    "print(f\"\\n1. Dim_eff comparison:\")\n",
    "print(f\"   Baseline: {base_dim:.1f}, Enhanced: {enh_dim:.1f}\")\n",
    "print(f\"   Change: {enh_dim - base_dim:+.1f} ({(enh_dim/base_dim - 1)*100:+.1f}%)\")\n",
    "print(f\"   Hypothesis (dim_eff increases with CoT): {'SUPPORTED' if enh_dim > base_dim else 'NOT SUPPORTED'}\")\n",
    "\n",
    "# Hypothesis 2: Lower entropy should correlate with correctness\n",
    "print(f\"\\n2. Entropy-correctness relationship:\")\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    r, p = correlation_results[condition]['entropy_correct']\n",
    "    direction = 'lower entropy → more correct' if r < 0 else 'higher entropy → more correct'\n",
    "    sig = 'significant' if p < 0.05 else 'not significant'\n",
    "    print(f\"   {condition}: r={r:.3f} ({direction}, {sig})\")\n",
    "\n",
    "# Hypothesis 3: Probe accuracy should exceed verbalized accuracy\n",
    "print(f\"\\n3. Recoverability (information in I vs verbalized output):\")\n",
    "for condition in ['baseline', 'enhanced']:\n",
    "    if condition in probe_results:\n",
    "        probe_acc = probe_results[condition]['mean']\n",
    "        verb_acc = probe_results[condition]['verbalized_accuracy']\n",
    "        gap = probe_acc - verb_acc\n",
    "        print(f\"   {condition}: Probe={probe_acc:.3f}, Verbalized={verb_acc:.3f}, Gap={gap:+.3f}\")\n",
    "\n",
    "# Accuracy improvement\n",
    "base_acc = np.mean(metrics_data['baseline']['correct'])\n",
    "enh_acc = np.mean(metrics_data['enhanced']['correct'])\n",
    "print(f\"\\n4. Overall accuracy:\")\n",
    "print(f\"   Baseline: {base_acc:.1%}\")\n",
    "print(f\"   Enhanced (CoT): {enh_acc:.1%}\")\n",
    "print(f\"   Improvement: {(enh_acc - base_acc)*100:+.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations\n",
    "\n",
    "Create publication-quality figures for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Publication-quality settings\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.titlesize': 13,\n",
    "})\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Color scheme\n",
    "COLORS = {\n",
    "    'baseline': '#1f77b4',  # Blue\n",
    "    'enhanced': '#ff7f0e',  # Orange\n",
    "    'correct': '#2ca02c',   # Green\n",
    "    'incorrect': '#d62728', # Red\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 1: METRICS COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# (A) Accuracy comparison\n",
    "ax = axes[0]\n",
    "conditions = ['Baseline', 'CoT']\n",
    "accuracies = [base_acc, enh_acc]\n",
    "colors = [COLORS['baseline'], COLORS['enhanced']]\n",
    "bars = ax.bar(conditions, accuracies, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('(A) Task Accuracy')\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "           f'{acc:.1%}', ha='center', fontsize=10)\n",
    "\n",
    "# (B) Entropy comparison\n",
    "ax = axes[1]\n",
    "data_to_plot = [\n",
    "    metrics_data['baseline']['entropy'],\n",
    "    metrics_data['enhanced']['entropy']\n",
    "]\n",
    "bp = ax.boxplot(data_to_plot, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor(COLORS['baseline'])\n",
    "bp['boxes'][1].set_facecolor(COLORS['enhanced'])\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "ax.set_xticklabels(['Baseline', 'CoT'])\n",
    "ax.set_ylabel('Intention Entropy (bits)')\n",
    "ax.set_title('(B) Intention Entropy $H_{int}(I)$')\n",
    "\n",
    "# (C) Dim_eff comparison\n",
    "ax = axes[2]\n",
    "data_to_plot = [\n",
    "    metrics_data['baseline']['dim_eff'],\n",
    "    metrics_data['enhanced']['dim_eff']\n",
    "]\n",
    "bp = ax.boxplot(data_to_plot, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor(COLORS['baseline'])\n",
    "bp['boxes'][1].set_facecolor(COLORS['enhanced'])\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "ax.set_xticklabels(['Baseline', 'CoT'])\n",
    "ax.set_ylabel('Effective Dimensionality')\n",
    "ax.set_title('(C) Intention Richness $dim_{eff}(I)$')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig1_metrics_comparison.pdf\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig1_metrics_comparison.png\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 2: ENTROPY VS CORRECTNESS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for idx, condition in enumerate(['baseline', 'enhanced']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    entropy_vals = metrics_data[condition]['entropy']\n",
    "    correct_vals = metrics_data[condition]['correct']\n",
    "    \n",
    "    # Separate by correctness\n",
    "    entropy_correct = [e for e, c in zip(entropy_vals, correct_vals) if c]\n",
    "    entropy_incorrect = [e for e, c in zip(entropy_vals, correct_vals) if not c]\n",
    "    \n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot([entropy_incorrect, entropy_correct], positions=[0, 1], showmeans=True)\n",
    "    parts['bodies'][0].set_facecolor(COLORS['incorrect'])\n",
    "    parts['bodies'][1].set_facecolor(COLORS['correct'])\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Incorrect', 'Correct'])\n",
    "    ax.set_ylabel('Intention Entropy (bits)')\n",
    "    ax.set_title(f'{condition.title()}')\n",
    "    \n",
    "    # Add statistics\n",
    "    r, p = correlation_results[condition]['entropy_correct']\n",
    "    ax.text(0.95, 0.95, f'r = {r:.3f}\\np = {p:.3f}',\n",
    "           transform=ax.transAxes, ha='right', va='top',\n",
    "           fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Entropy Distribution by Answer Correctness', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig2_entropy_correctness.pdf\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig2_entropy_correctness.png\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 3: RECOVERABILITY GAP\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "# Get probe and verbalized accuracies\n",
    "probe_accs = [probe_results[c]['mean'] for c in ['baseline', 'enhanced']]\n",
    "verb_accs = [probe_results[c]['verbalized_accuracy'] for c in ['baseline', 'enhanced']]\n",
    "probe_stds = [probe_results[c]['std'] for c in ['baseline', 'enhanced']]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, probe_accs, width, label='Probe on I (Recoverability)',\n",
    "               color=COLORS['enhanced'], alpha=0.8, yerr=probe_stds, capsize=5)\n",
    "bars2 = ax.bar(x + width/2, verb_accs, width, label='Verbalized Output',\n",
    "               color=COLORS['baseline'], alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Baseline', 'CoT'])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Information Recovery: Probe vs. Verbalized Output')\n",
    "\n",
    "# Add gap annotations\n",
    "for i, (p, v) in enumerate(zip(probe_accs, verb_accs)):\n",
    "    gap = p - v\n",
    "    mid_y = (p + v) / 2\n",
    "    ax.annotate(f'Gap: {gap:+.1%}', xy=(i, mid_y), fontsize=9,\n",
    "               ha='center', va='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig3_recoverability_gap.pdf\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig3_recoverability_gap.png\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIGURE 4: CORRELATION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, condition in enumerate(['baseline', 'enhanced']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Build correlation matrix\n",
    "    data = metrics_data[condition]\n",
    "    variables = ['entropy', 'dim_eff', 'output_length', 'correct']\n",
    "    n = len(variables)\n",
    "    corr_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i, var_i in enumerate(variables):\n",
    "        for j, var_j in enumerate(variables):\n",
    "            vals_i = np.array(data[var_i]).astype(float)\n",
    "            vals_j = np.array(data[var_j]).astype(float)\n",
    "            corr_matrix[i, j] = np.corrcoef(vals_i, vals_j)[0, 1]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            val = corr_matrix[i, j]\n",
    "            color = 'white' if abs(val) > 0.5 else 'black'\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                   color=color, fontsize=10)\n",
    "    \n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    labels = ['Entropy', 'Dim_eff', 'Length', 'Correct']\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_title(f'{condition.title()}')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6)\n",
    "cbar.set_label('Correlation')\n",
    "\n",
    "plt.suptitle('Correlation Matrix: Intention Metrics vs. Performance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "if CONFIG['save_figures']:\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig4_correlation_matrix.pdf\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/fig4_correlation_matrix.png\", \n",
    "                bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "Save all results for later analysis and paper inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TABLE\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Entropy (mean)', 'Entropy (std)', \n",
    "               'Dim_eff (mean)', 'Dim_eff (std)',\n",
    "               'Probe Accuracy', 'Recoverability Gap'],\n",
    "    'Baseline': [\n",
    "        f\"{np.mean(metrics_data['baseline']['correct']):.1%}\",\n",
    "        f\"{np.mean(metrics_data['baseline']['entropy']):.2f}\",\n",
    "        f\"{np.std(metrics_data['baseline']['entropy']):.2f}\",\n",
    "        f\"{np.mean(metrics_data['baseline']['dim_eff']):.1f}\",\n",
    "        f\"{np.std(metrics_data['baseline']['dim_eff']):.1f}\",\n",
    "        f\"{probe_results['baseline']['mean']:.3f} ± {probe_results['baseline']['std']:.3f}\",\n",
    "        f\"{probe_results['baseline']['mean'] - probe_results['baseline']['verbalized_accuracy']:+.3f}\"\n",
    "    ],\n",
    "    'CoT': [\n",
    "        f\"{np.mean(metrics_data['enhanced']['correct']):.1%}\",\n",
    "        f\"{np.mean(metrics_data['enhanced']['entropy']):.2f}\",\n",
    "        f\"{np.std(metrics_data['enhanced']['entropy']):.2f}\",\n",
    "        f\"{np.mean(metrics_data['enhanced']['dim_eff']):.1f}\",\n",
    "        f\"{np.std(metrics_data['enhanced']['dim_eff']):.1f}\",\n",
    "        f\"{probe_results['enhanced']['mean']:.3f} ± {probe_results['enhanced']['std']:.3f}\",\n",
    "        f\"{probe_results['enhanced']['mean'] - probe_results['enhanced']['verbalized_accuracy']:+.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS TABLE (for paper)\")\n",
    "print(\"=\"*60)\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_table.to_csv(f\"{CONFIG['output_dir']}/results_summary.csv\", index=False)\n",
    "print(f\"\\n✓ Results saved to {CONFIG['output_dir']}/results_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT LATEX TABLE\n",
    "# =============================================================================\n",
    "\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Experiment 4.1 Results: Intention Metrics vs. Reasoning Accuracy}\n",
    "\\label{tab:exp41_results}\n",
    "\\begin{tabular}{lcc}\n",
    "\\hline\n",
    "\\textbf{Metric} & \\textbf{Baseline} & \\textbf{CoT} \\\\\n",
    "\\hline\n",
    "\"\"\" + f\"\"\"\n",
    "Accuracy & {np.mean(metrics_data['baseline']['correct']):.1%} & {np.mean(metrics_data['enhanced']['correct']):.1%} \\\\\\\\\n",
    "$H_{{int}}(I)$ (mean $\\pm$ std) & {np.mean(metrics_data['baseline']['entropy']):.2f} $\\pm$ {np.std(metrics_data['baseline']['entropy']):.2f} & {np.mean(metrics_data['enhanced']['entropy']):.2f} $\\pm$ {np.std(metrics_data['enhanced']['entropy']):.2f} \\\\\\\\\n",
    "$dim_{{eff}}(I)$ (mean $\\pm$ std) & {np.mean(metrics_data['baseline']['dim_eff']):.1f} $\\pm$ {np.std(metrics_data['baseline']['dim_eff']):.1f} & {np.mean(metrics_data['enhanced']['dim_eff']):.1f} $\\pm$ {np.std(metrics_data['enhanced']['dim_eff']):.1f} \\\\\\\\\n",
    "Probe Accuracy & {probe_results['baseline']['mean']:.3f} $\\pm$ {probe_results['baseline']['std']:.3f} & {probe_results['enhanced']['mean']:.3f} $\\pm$ {probe_results['enhanced']['std']:.3f} \\\\\\\\\n",
    "Recoverability Gap & {probe_results['baseline']['mean'] - probe_results['baseline']['verbalized_accuracy']:+.3f} & {probe_results['enhanced']['mean'] - probe_results['enhanced']['verbalized_accuracy']:+.3f} \\\\\\\\\n",
    "\"\"\" + r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_table)\n",
    "\n",
    "# Save to file\n",
    "with open(f\"{CONFIG['output_dir']}/results_table.tex\", 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"✓ LaTeX table saved to {CONFIG['output_dir']}/results_table.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE FULL RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "full_results = {\n",
    "    'config': CONFIG,\n",
    "    'metrics_data': {\n",
    "        condition: {\n",
    "            k: [float(v) if isinstance(v, (np.floating, np.integer)) else v for v in vals]\n",
    "            for k, vals in data.items()\n",
    "        }\n",
    "        for condition, data in metrics_data.items()\n",
    "    },\n",
    "    'correlation_results': {\n",
    "        condition: {\n",
    "            k: {'r': float(v[0]), 'p': float(v[1])}\n",
    "            for k, v in corrs.items()\n",
    "        }\n",
    "        for condition, corrs in correlation_results.items()\n",
    "    },\n",
    "    'probe_results': probe_results\n",
    "}\n",
    "\n",
    "with open(f\"{CONFIG['output_dir']}/full_results.json\", 'w') as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Full results saved to {CONFIG['output_dir']}/full_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook implemented Experiment 4.1 from the Intention Collapse framework paper, testing whether intention metrics correlate with reasoning accuracy in LLMs.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Effective Dimensionality**: Chain-of-thought reasoning [increases/decreases] the effective dimensionality of internal representations\n",
    "\n",
    "2. **Intention Entropy**: Lower entropy [correlates/does not correlate] with correct answers\n",
    "\n",
    "3. **Recoverability**: Linear probes on pre-collapse activations achieve [higher/lower] accuracy than verbalized outputs, suggesting information is [lost/preserved] during collapse\n",
    "\n",
    "### Files Generated:\n",
    "- `results/fig1_metrics_comparison.pdf` - Main comparison figure\n",
    "- `results/fig2_entropy_correctness.pdf` - Entropy vs correctness\n",
    "- `results/fig3_recoverability_gap.pdf` - Probe vs verbalized accuracy\n",
    "- `results/fig4_correlation_matrix.pdf` - Full correlation matrix\n",
    "- `results/results_summary.csv` - Summary statistics\n",
    "- `results/results_table.tex` - LaTeX table for paper\n",
    "- `results/full_results.json` - Complete results data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "import os\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(os.listdir(CONFIG['output_dir'])):\n",
    "    filepath = os.path.join(CONFIG['output_dir'], f)\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(f\"  {f} ({size/1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
